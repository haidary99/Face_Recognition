{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this version, I'm trying to see if I can confirm if an employee is present only if he is showing in the frames (video feed) for about 3 seconds to make it more robust."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=IT53xBR1A7M&list=PLqUHmcsDDjLjFSo4iumsXGhQoaNu-aXUF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This jupyter notebook is used to detect faces in images (using MTCNN) stored in `photos` directory and extract their features (using InceptionResnetV1) and store them in `data.pt` file. This file is then loaded later in order to extract the features of faces stored in it and compare it with the face(s) detected in each live frame from a camera stream. If there is a match according to some threshold, then it will label the person in the frame.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing MTCNN and InceptionResnetv1\n",
    "\n",
    "mtcnn0 = MTCNN(image_size=240, margin=0, keep_all=False, min_face_size=40) # keep_all = False means if one img contains many faces then it will keep only 1 face of those\n",
    "mtcnn = MTCNN(image_size=240, margin=0, keep_all=True, min_face_size=300) # keep_all = True keeps all faces in a form of list, Also changed mmin_face_size to 150 to ignore small faces in frame which might be difficult to classify\n",
    "resnet = InceptionResnetV1(pretrained='vggface2').eval() # creating an instance of pre-trained InceptionResnetV1 model trained on the VGGFace2 dataset to extract embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Loei_Haidari', 1: 'Najwa', 2: 'Taher_Haidari', 3: 'Yousef_Haidari'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data from directory\n",
    "\n",
    "dataset = datasets.ImageFolder('photos') # photos directory path\n",
    "idx_to_class = {i:c for c,i in dataset.class_to_idx.items()} # new dictionary of indecis and class names\n",
    "idx_to_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1614da040>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def collate_fn(x):\n",
    "    return x[0]\n",
    "\n",
    "loader = DataLoader(dataset, collate_fn=collate_fn)\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list = [] # list of names corresponding to cropped photos\n",
    "embedding_list = [] # list of embedding matrix after conversion from cropped faces to embedding matrix using resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img, idx in loader: # img is in PIL format, here we are looping thru the images in the photos directory (loader)\n",
    "    face, prob = mtcnn0(img, return_prob=True) # img passed into mtcnn0, face is the cropped face (note: in mtcnn0 it will return only ONE face)\n",
    "    if face is not None and prob>0.92: # if face is not None means that there's a face that exists \n",
    "        emb = resnet(face.unsqueeze(0)) # pass the face into resnet, unsqueeze b/c resnet expects 4 dimensions (dimension of batch included). Result of this is an embedding\n",
    "        embedding_list.append(emb.detach()) # .detach() to make requires_grad false\n",
    "        name_list.append(idx_to_class[idx])\n",
    "\n",
    "# save data\n",
    "data = [embedding_list, name_list] # make a new list of the previous 2 lists\n",
    "torch.save(data, 'data.pt') # saving to make this code reusable without having to go thru the previous loops and steps every single time which is very costly"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using webcam to recognize the face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attendance information before program starts:\n",
      "             Name  status arrival_date arrival_time departure_date  \\\n",
      "0    Loei Haidari  absent                                            \n",
      "1           Najwa  absent                                            \n",
      "2   Taher Haidari  absent                                            \n",
      "3  Yousef Haidari  absent                                            \n",
      "\n",
      "  departure_time  \n",
      "0                 \n",
      "1                 \n",
      "2                 \n",
      "3                 \n",
      "\n",
      "[INFO] Yousef Haidari arrived at: 20:57:50\n",
      "\n",
      "Attendance information after program stopped:\n",
      "             Name   status arrival_date arrival_time departure_date  \\\n",
      "0    Loei Haidari   absent                                            \n",
      "1           Najwa   absent                                            \n",
      "2   Taher Haidari   absent                                            \n",
      "3  Yousef Haidari  present   2023-07-23     20:57:50                  \n",
      "\n",
      "  departure_time  \n",
      "0                 \n",
      "1                 \n",
      "2                 \n",
      "3                 \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Loading data.pt file\n",
    "load_data = torch.load('data.pt')\n",
    "embedding_list = load_data[0] \n",
    "name_list = load_data[1]\n",
    "\n",
    "cam = cv2.VideoCapture(0) # initializing the camera from CV, 0 means the default webcam in the device\n",
    "\n",
    "names = [name.replace('_', ' ') for name in name_list]\n",
    "attendance_dict = {name: {\"status\": \"absent\", \"arrival_date\": \"\", \"arrival_time\": \"\", \"departure_date\": \"\", \"departure_time\": \"\"} for name in names} # Initialize attendance dictionary which has all the names in the db as keys and another dictionary as value\n",
    "\n",
    "# Initialize counter and maximum consecutive frames\n",
    "consecutive_frames_dict = {name: 0 for name in names} # all names in db as keys and 0 for their values\n",
    "max_consecutive_frames = 50 # increase/decrease this to modify time needed to take attendance\n",
    "\n",
    "# Convert the attendance_dict to a Pandas DataFrame\n",
    "df_attendance = pd.DataFrame.from_dict(attendance_dict, orient='index')\n",
    "\n",
    "# Reset the index to get the names as a column instead of an index\n",
    "df_attendance.reset_index(inplace=True)\n",
    "df_attendance.rename(columns={'index': 'Name'}, inplace=True)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(\"\\nAttendance information before program starts:\")\n",
    "print(df_attendance)\n",
    "\n",
    "# Initialize a dictionary to keep track of the last frame number where an employee's status was changed to \"present\"\n",
    "last_present_frame_dict = {name: -1 for name in names}\n",
    "\n",
    "while True: # read all the frames in the video\n",
    "    ret, frame = cam.read() # return is true or false, if the webcam succesfully captures an image then it is true otherwise false. frame reperesnts a single frame (image) captured from the webcam using VideoCapture object\n",
    "    if not ret:\n",
    "        print(\"\\nFail to grab frame, try again\")\n",
    "        break\n",
    "    \n",
    "    img = Image.fromarray(frame) # Image class is part of PIL, .fromarray() is a method that creates an \"Image\" object from a numpy array \n",
    "    img_cropped_list, prob_list = mtcnn(img, return_prob=True) # pass the image into mtcnn, mtcnn will return multiple faces if the image contain multiple faces, also return all the probabilities for all the faces\n",
    "\n",
    "    face_detected: bool\n",
    "\n",
    "    if img_cropped_list is not None:\n",
    "        # If there's a face, make face_detected = true and increment the counter of that identity\n",
    "        face_detected = True\n",
    "        try:\n",
    "            consecutive_frames_dict[name] += 1\n",
    "        except:\n",
    "            # Handle the NameError here\n",
    "            print(\"NameError occured\")\n",
    "            pass\n",
    "    \n",
    "    elif img_cropped_list is None:\n",
    "        # If there's no face, make face_detected = false and re-initialize the counter back to 0. This is done to make sure that a face is detected in CONSECUTIVE frames if not, the counter is back to 0\n",
    "        face_detected = False\n",
    "        try:\n",
    "            consecutive_frames_dict[name] = 0\n",
    "        except NameError:\n",
    "            # Handle the NameError here\n",
    "            print(\"NameError occured\")\n",
    "            pass\n",
    "\n",
    "    if img_cropped_list is not None: # if the image has at least one face\n",
    "        boxes, _ = mtcnn.detect(img) # return the boxes of faces \n",
    "        # print(boxes[0][0])\n",
    "\n",
    "        for i, prob in enumerate(prob_list): # loop thru the prob list\n",
    "            if prob > 0.90: \n",
    "                emb = resnet(img_cropped_list[i].unsqueeze(0)).detach()\n",
    "                \n",
    "                dist_list = [] # for the distance between the current embedding and embeddings of faces in the photos directory (similarity). Minimum distance is used to identify the person\n",
    "\n",
    "                for idx, emd_db in enumerate(embedding_list):\n",
    "                    dist = torch.dist(emb, emd_db).item() # calc. distance betweem current embedding and embeddings stored embedding_list\n",
    "                    dist_list.append(dist)\n",
    "\n",
    "                # print(f\"Minimum distance array:\\n{dist_list}\")\n",
    "                min_dist = min(dist_list) # get minimum dist value\n",
    "                # formatted_min_dist = f'{min_dist:.4f}' # used for printing on the frame\n",
    "                min_dist_idx = dist_list.index(min_dist) # get minimum dist index (where the min dist is located in the array)\n",
    "                name = names[min_dist_idx] # get name corresponding to minium dist ##########\n",
    "                name = name.replace('_', ' ')\n",
    "\n",
    "                box = boxes[i]\n",
    " \n",
    "                if name in consecutive_frames_dict: # when a face is detected, we retrieve the recognized name\n",
    "                    if consecutive_frames_dict[name] >= max_consecutive_frames: \n",
    "                        # If the counter reaches or exceeds the max_consecutive_frames, we update the attendance status of the person in the attendance_dict from \"absent\" to \"present\"\n",
    "                        if attendance_dict[name][\"status\"] == \"absent\":\n",
    "                            \n",
    "                            current_date = time.strftime(\"%Y-%m-%d\")\n",
    "                            current_time = time.strftime(\"%H:%M:%S\")\n",
    "\n",
    "                            print(f\"\\n[INFO] {name} arrived at: {current_time}\")\n",
    "\n",
    "                            # Update the date and specific time of face detection\n",
    "                            attendance_dict[name][\"arrival_time\"] = current_time\n",
    "                            attendance_dict[name][\"arrival_date\"] = current_date\n",
    "\n",
    "                            # Update status\n",
    "                            attendance_dict[name][\"status\"] = \"present\"\n",
    "\n",
    "                            # Update the last_present_frame_dict to keep track of the frame number\n",
    "                            last_present_frame_dict[name] = consecutive_frames_dict[name]\n",
    "\n",
    "                            # Update the DataFrame with the latest attendance information\n",
    "                            df_attendance[\"status\"] = df_attendance[\"Name\"].map(lambda name: attendance_dict[name][\"status\"])\n",
    "                            df_attendance[\"arrival_date\"] = df_attendance[\"Name\"].map(lambda name: attendance_dict[name][\"arrival_date\"])\n",
    "                            df_attendance[\"arrival_time\"] = df_attendance[\"Name\"].map(lambda name: attendance_dict[name][\"arrival_time\"])\n",
    "\n",
    "                        elif consecutive_frames_dict[name] - last_present_frame_dict[name] >= (max_consecutive_frames):\n",
    "                            # If the current frame number minus the last_present_frame is greater than or equal to max_consecutive_frames,\n",
    "                            # we change the status back to \"absent\" after a certain number of frames have passed since the employee's last presence.\n",
    "                            if attendance_dict[name][\"status\"] == \"present\":\n",
    "\n",
    "                                current_date = time.strftime(\"%Y-%m-%d\")\n",
    "                                current_time = time.strftime(\"%H:%M:%S\")\n",
    "\n",
    "                                print(f\"\\n[INFO] {name} departed at: {current_time}\")\n",
    "\n",
    "                                # Update the date and specific time of face detection\n",
    "                                attendance_dict[name][\"departure_time\"] = current_time\n",
    "                                attendance_dict[name][\"departure_date\"] = current_date\n",
    "\n",
    "                                attendance_dict[name][\"status\"] = \"departed\"\n",
    "\n",
    "                                # Update the DataFrame with the latest attendance information\n",
    "                                df_attendance[\"status\"] = df_attendance[\"Name\"].map(lambda name: attendance_dict[name][\"status\"])\n",
    "                                df_attendance[\"departure_date\"] = df_attendance[\"Name\"].map(lambda name: attendance_dict[name][\"departure_date\"])\n",
    "                                df_attendance[\"departure_time\"] = df_attendance[\"Name\"].map(lambda name: attendance_dict[name][\"departure_time\"])\n",
    "                else:\n",
    "                    consecutive_frames_dict[name] = 0\n",
    "\n",
    "                original_frame = frame.copy() # storing a copy of frame before drawing on it\n",
    "        \n",
    "                if min_dist<0.90:\n",
    "                    # frame = cv2.putText(frame, name+' '+str(formatted_min_dist), (int(box[0])-5, int(box[1])-5), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2, cv2.LINE_AA)\n",
    "                    # Display employee name and status on the frame\n",
    "                    if attendance_dict[name][\"status\"] == \"present\":\n",
    "                        status_color = (0, 255, 0)\n",
    "                        status_text = \"Arrived\"\n",
    "                        frame = cv2.putText(frame, f\"{name} - {status_text}\", (int(box[0])-5, int(box[1])-12), cv2.FONT_HERSHEY_SIMPLEX, 1, status_color, 2, cv2.LINE_AA)\n",
    "                        frame = cv2.rectangle(frame, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), status_color, 3)\n",
    "\n",
    "                    elif attendance_dict[name][\"status\"] == \"departed\":\n",
    "                        status_color = (255, 0, 0)\n",
    "                        status_text = \"Departed\"\n",
    "                        frame = cv2.putText(frame, f\"{name} - {status_text}\", (int(box[0])-5, int(box[1])-12), cv2.FONT_HERSHEY_SIMPLEX, 1, status_color, 2, cv2.LINE_AA)\n",
    "                        frame = cv2.rectangle(frame, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), status_color, 3)\n",
    "\n",
    "                    else:\n",
    "                        status_color = (0, 0, 255)\n",
    "                        status_text = \"Absent\"\n",
    "                        frame = cv2.putText(frame, f\"{name} - {status_text}\", (int(box[0])-5, int(box[1])-12), cv2.FONT_HERSHEY_SIMPLEX, 1, status_color, 2, cv2.LINE_AA)\n",
    "                        frame = cv2.rectangle(frame, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), status_color, 3)\n",
    "\n",
    "    cv2.imshow(\"IMG\", frame)\n",
    "\n",
    "    k = cv2.waitKey(1)\n",
    "    # Press q or esc to quit, space bar to add & save new image\n",
    "    if k == ord('q'):\n",
    "        break # Quit the program\n",
    "\n",
    "    elif k == 27: # 27 is for esc key\n",
    "        break\n",
    "\n",
    "    elif k == 32: # 32 is the ASCII of space bar\n",
    "        print('Enter your name: ')\n",
    "        name = input()\n",
    "        if name == \"\": ###### I ADDED THESE 2 LINES IN CASE INPUT WAS EMPTY IT DOESN'T SAVE TEH IMAGE\n",
    "            continue\n",
    "\n",
    "        # Create directory if class/person does not exist in photos directory\n",
    "        if not os.path.exists('photos/'+name):\n",
    "            os.mkdir('photos/'+name) \n",
    "\n",
    "        img_name = f\"photos/{name}/{int(time.time())}.jpg\"\n",
    "        cv2.imwrite(img_name, original_frame)\n",
    "        print(f\"saved: {img_name}\")\n",
    "\n",
    "cam.release()\n",
    "\n",
    "# Print the DataFrame\n",
    "print(\"\\nAttendance information after program stopped:\")\n",
    "print(df_attendance)\n",
    "\n",
    "# print()\n",
    "# print(attendance_dict)\n",
    "\n",
    "# # Export the DataFrame to a CSV or xlsx (excel) file\n",
    "# excel_file = 'attendance.xlsx'\n",
    "# # Use  .to_excel() or .to_csv() method to export dataframe\n",
    "# df_attendance.to_excel(excel_file, index=False) # index=False argument ensures that the row indices are not included in the CSV file.\n",
    "# print(f\"\\n[INFO] Attendance data has been exported to '{excel_file}' successfully.\")\n",
    "\n",
    "for i in range(1):\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n",
    "\n",
    "# OR the below method, but below method requires one moqre key press to destroy the window while the above loop automatically closes the window without extra key press\n",
    "\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()\n",
    "# cv2.waitKey(1) # This is the only extra line u need on top of windows os uses to work properly on mac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !jupyter nbconvert --to script face_reco.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
